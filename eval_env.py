from robot_env import RobotEnv
from downsampler import DownSampler
import time

import numpy as np
from sklearn.feature_selection import mutual_info_regression


class EvalEnv(RobotEnv):
    def __init__(self, case=None):
        """ case can be 'RANSAC_MI' or 'BAYES_SWARM' currently"""
        super().__init__()
        self.case = case

        #Downsampler
        self.downsampler = DownSampler()
        
    def step(self, actions=None):
        """
        steps:
        1. The action is received and masked.
        2. If any new obs were filtered, then the new obs are all measured so the signals can be used later for fitting the GP
        3. After {D_old_filt, D_new_filt} is created, D_new is chosen from the sampler. Then, a new observation is created.
        4. The Y_mean and Y_std from the obs is used for calculating the reward.

        inputs:
        -action: Action provided by the CNN policy, consists of N "actions" after masking, which are chosen indices for the robot observation. N <= action cap.
        
        outputs:
        -observation: A dict consisting of three matrices: Y_mean, Y_std and new_obs Generated by the sampler. A mask is also included.
        -reward: Reward calculated by the Y_mean and/or Y_std matrices.
        -done: Returns True if a certain number of steps has passed AND the agent stays below a poor state threshold, else False.
        -info: Nothing, as of now.
        """
        
        self.iter += 1 
        
        
        D_new_full = self.source.measure_signal(self.D_new)
        D_all = np.vstack((self.D_old, D_new_full))

        #Downsampler filtered dataset
        step0 = time.time()

        if self.case == "RANSAC_MI":
            self.D_old = self.downsampler.mi_downsample(D_all)
        elif self.case == "BAYES_SWARM":
            self.D_old = self.downsampler.bayes_downsample(D_all)
        elif actions:
            while len(action) == 1:
                action = action[0]
            action = action[:self.D_old.shape[0] + self.D_new.shape[0]] # Used in place of masking
            max_idxs = np.argpartition(action, -self.sample_cap)[-self.sample_cap:]
            self.D_old = D_all[max_idxs]


        downsample_time = time.time() - step0
        
        self.D_new = self.sampler.create_new_obs(self.D_old)
        obs, mixed_state = self.create_obs()

        # reward = self.calculate_reward_entropy(mixed_state)
        reward = self.calculate_reward_mi()

        done = False if (self.iter < self.termination_iter) else True

        info = {"downsample_time": downsample_time,
                "poor_states": self.poor_states}

        return obs, reward, done, info

    def calculate_eval_reward(self, d1, d2):
        """
        steps:
        1. Calculate the MI for both d1 and d2
        2. Compare them. We want d1 to have a higher MI than d2.

        inputs:
        d1: Filtered dataset returned by the CNN
        d2: Filtered dataset returned by the information gain downsampler

        outputs:
        reward: an evaluation of the two inputs
        """

        mi_1 = mutual_info_regression(d1[:,:2], d1[:,2])
        mi_2 = mutual_info_regression(d2[:,:2], d2[:,2])

        reward = np.sum(mi_1 - mi_2)

        return reward

    


